ğŸ–±ï¸ AI Mouse â€” Real-Time Hand Gesture Control
(Computer Vision & Hybrid Machine Learning â€” Research Project)

AI Mouse is a real-time computer vision research project that explores mouse control using hand gestures captured via a webcam.

The system combines MediaPipe hand tracking with a hybrid machine learning approach (KNN + Random Forest) to achieve low-latency, stable, and adaptive gesture recognition.

This project focuses on system design, real-time performance, and practical ML decision-making, rather than deep learning or production deployment.

âœ¨ Features

ğŸ¥ Real-time hand tracking using a standard webcam

âœ‹ Gesture-based mouse control

Move

Scroll

Click

ğŸ§  Hybrid ML decision system

KNN for fast online adaptation

Random Forest for confidence stabilization

ğŸ” Incremental training during runtime

ğŸ¯ Confidence-based action execution

ğŸ“¦ Modular and clean code architecture

ğŸ–¥ï¸ Fully offline operation
```text
ğŸ—ï¸ Project Architecture
AI_MOUSE/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ config.py        # System & gesture configuration
â”‚   â”œâ”€â”€ features.py      # Hand landmark feature extraction
â”‚   â”œâ”€â”€ model.py         # Hybrid KNN + RandomForest logic
â”‚   â”œâ”€â”€ actions.py       # Mouse / scroll / click execution
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ main.py              # Real-time camera loop & orchestration
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
```
ğŸ§  Why Hybrid ML (Not Deep Learning)?

This problem demands:

âš¡ Ultra-low latency

ğŸ§© Online / incremental learning

ğŸ§  Interpretability and control

Deep learning models introduce unnecessary latency and complexity for this use case.

Instead, this project uses:

KNN for instant adaptation to new gestures

Random Forest for stabilizing predictions

Temporal buffering to reduce jitter

This design makes the system fast, robust, and practical for real-time interaction.

ğŸ§ª Gestures
Key	Gesture	Action
1	MOVE	Mouse movement
2	SCROLL	Scroll up / down
3	CLICK	Mouse click
4	IDLE	No action

Gestures are trained live during runtime and are user-specific.

â–¶ï¸ How to Run
1ï¸âƒ£ Create & activate virtual environment
```text
py -3.10 -m venv .venv
.venv\Scripts\activate
```
2ï¸âƒ£ Install dependencies
```text
pip install -r requirements.txt
```
3ï¸âƒ£ Run the system
```text
python main.py
```
âŒ¨ï¸ Controls
```text
Key	Action
1 / 2 / 3 / 4	Train gesture
s	Save trained data
r	Reset calibration
ESC	Exit program
âš ï¸ Safety Warning
```
This project disables PyAutoGUI failsafe for smoother control:

pyautogui.FAILSAFE = False


âš ï¸ If the mouse behaves unexpectedly:
```text
Press ESC

Or Alt + Tab
```
Or close the OpenCV window immediately

Use with caution. This behavior is intentional for experimentation.

ğŸ“Œ Notes

Trained gesture data (.pkl) is user-specific and intentionally ignored in GitHub

Webcam is required

Designed and tested on Windows

Not intended for accessibility or production use

ğŸš€ Future Improvements

Gesture visualization overlays

Dynamic sensitivity tuning

GUI-based training interface

Comparative study with deep learning models

ğŸ§  Key Takeaway

AI Mouse demonstrates that classical machine learning combined with computer vision and careful system design can outperform deep learning for real-time, interactive humanâ€“computer interfaces.

ğŸ“œ License

This project is shared for educational and experimental purposes only.
